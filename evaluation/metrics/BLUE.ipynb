{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:32:19.020279Z",
     "start_time": "2025-05-29T16:32:18.990506Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "accd83118a527fe1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:32:19.324298Z",
     "start_time": "2025-05-29T16:32:19.271981Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")",
   "id": "7db21715622f80ef",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:32:19.666901Z",
     "start_time": "2025-05-29T16:32:19.633738Z"
    }
   },
   "cell_type": "code",
   "source": "df.columns",
   "id": "ed52fa024df2c812",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#', 'Question', 'Quality of the question (Low, Medium, High)',\n",
       "       'Expected Answer', 'Initial Answer from Base Model (Mistral 7B)',\n",
       "       'Initial Answer from LLM (gpt-4o-mini) ',\n",
       "       'Finetuned Mistral Model 1 (250 Tokens)',\n",
       "       'Finetuned Mistral Model 1 (100 Tokens)',\n",
       "       'BLUE Score  (Base Mistral 7B  vs  gpt-4o-mini  vs  Finetuned Mistral Model 1)',\n",
       "       'Finetuned Mistral Model 2 (250 Tokens)',\n",
       "       'Finetuned Mistral Model 2 (100 Tokens)',\n",
       "       'BLUE Score  (Base Mistral 7B  vs  gpt-4o-mini  vs  Finetuned Mistral Model 2)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BLUE score with sacrebleu library",
   "id": "a7dc4a20c5ef945f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:35:00.128267Z",
     "start_time": "2025-05-29T16:35:00.121615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sacrebleu\n",
    "\n",
    "references = [[\"The cat is on the mat.\"]]  # list of reference lists\n",
    "hypotheses = [\"The cat is on the mat\"]        # list of system outputs\n",
    "\n",
    "score = sacrebleu.corpus_bleu(hypotheses, references)\n",
    "print(score.score)  # Output: BLEU score as float\n"
   ],
   "id": "282af3e1b37601e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.64817248906144\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1 BLUE score",
   "id": "2264791adfb0f89a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:45:30.685666Z",
     "start_time": "2025-05-29T16:45:29.460775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sacrebleu\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation if needed\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "\n",
    "# Clean and rename columns\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 1 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Limit to first 60 samples\n",
    "df = df.head(60)\n",
    "\n",
    "# Prepare for BLEU scoring\n",
    "references = []\n",
    "fine_tuned_candidates = []\n",
    "base_model_candidates = []\n",
    "gpt4o_candidates = []\n",
    "\n",
    "# Output file\n",
    "with open(\"bleu_scores_sacrebleu_model_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    fine_tuned_better_base = 0\n",
    "    fine_tuned_better_gpt = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    fine_tuned_scores = []\n",
    "    base_model_scores = []\n",
    "    gpt4o_scores = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # Clean text fields\n",
    "        reference = clean_text(row.get('reference', ''))\n",
    "        base_model = clean_text(row.get('base_model', ''))\n",
    "        gpt4o = clean_text(row.get('gpt4omini', ''))\n",
    "        fine_tuned = clean_text(row.get('fine_tuned', ''))\n",
    "\n",
    "        # Skip empty or invalid rows\n",
    "        if reference == \"\" or (base_model == \"\" and gpt4o == \"\" and fine_tuned == \"\"):\n",
    "            continue\n",
    "\n",
    "        # Compute sentence-level BLEU (0–1)\n",
    "        bleu_fine = sacrebleu.sentence_bleu(fine_tuned, [reference]).score / 100\n",
    "        bleu_base = sacrebleu.sentence_bleu(base_model, [reference]).score / 100\n",
    "        bleu_gpt = sacrebleu.sentence_bleu(gpt4o, [reference]).score / 100\n",
    "\n",
    "        fine_tuned_scores.append(bleu_fine)\n",
    "        base_model_scores.append(bleu_base)\n",
    "        gpt4o_scores.append(bleu_gpt)\n",
    "\n",
    "        if bleu_fine > bleu_base:\n",
    "            fine_tuned_better_base += 1\n",
    "        if bleu_fine > bleu_gpt:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        # Add for corpus-level BLEU\n",
    "        references.append(reference)\n",
    "        fine_tuned_candidates.append(fine_tuned)\n",
    "        base_model_candidates.append(base_model)\n",
    "        gpt4o_candidates.append(gpt4o)\n",
    "\n",
    "        # Write per-sample BLEU scores\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: BLEU = {bleu_fine:.4f}\\n\")\n",
    "        f.write(f\"base_model: BLEU = {bleu_base:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: BLEU = {bleu_gpt:.4f}\\n\\n\")\n",
    "\n",
    "    # Corpus-level BLEU\n",
    "    bleu_fine_total = sacrebleu.corpus_bleu(fine_tuned_candidates, [references]).score / 100\n",
    "    bleu_base_total = sacrebleu.corpus_bleu(base_model_candidates, [references]).score / 100\n",
    "    bleu_gpt_total = sacrebleu.corpus_bleu(gpt4o_candidates, [references]).score / 100\n",
    "\n",
    "    # Write summary results\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model (Mistral 7B): {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    f.write(\"Average Sentence BLEU Scores (0–1):\\n\")\n",
    "    f.write(f\"fine_tuned: {np.mean(fine_tuned_scores):.4f}\\n\")\n",
    "    f.write(f\"base_model: {np.mean(base_model_scores):.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {np.mean(gpt4o_scores):.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus BLEU Scores (0–1):\\n\")\n",
    "    f.write(f\"fine_tuned: {bleu_fine_total:.4f}\\n\")\n",
    "    f.write(f\"base_model: {bleu_base_total:.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {bleu_gpt_total:.4f}\\n\")\n",
    "\n",
    "print(\"BLEU scores (0–1) for first 60 samples saved to bleu_scores_sacrebleu_model_1.txt\")\n"
   ],
   "id": "ba804052cc22828f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores (0–1) for first 60 samples saved to bleu_scores_sacrebleu_model_1.txt\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2 BLUE score",
   "id": "8cf3298a78119c32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:51:15.753683Z",
     "start_time": "2025-05-29T16:51:15.494529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load your data (replace with your actual file)\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 1 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation if needed\n",
    "    return text\n",
    "\n",
    "# Prepare smoothing function to avoid BLEU=0 for short sentences\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Lists for corpus BLEU calculation\n",
    "corpus_references = []\n",
    "corpus_base_model = []\n",
    "corpus_gpt4o = []\n",
    "corpus_fine_tuned = []\n",
    "\n",
    "# Open file for writing results\n",
    "with open(\"bleu_scores_nltk_model_1.txt\", \"w\") as f:\n",
    "\n",
    "    fine_tuned_better_base = 0\n",
    "    fine_tuned_better_gpt = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    fine_tuned_scores = []\n",
    "    base_model_scores = []\n",
    "    gpt4o_scores = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # Handle missing or non-string data and clean text\n",
    "        reference = clean_text(row['reference']) if pd.notna(row['reference']) else \"\"\n",
    "        base_model = clean_text(row['base_model']) if pd.notna(row['base_model']) else \"\"\n",
    "        gpt4o = clean_text(row['gpt4omini']) if pd.notna(row['gpt4omini']) else \"\"\n",
    "        fine_tuned = clean_text(row['fine_tuned']) if pd.notna(row['fine_tuned']) else \"\"\n",
    "\n",
    "        # Skip if reference or all models are empty\n",
    "        if reference == \"\" or (base_model == \"\" and gpt4o == \"\" and fine_tuned == \"\"):\n",
    "            continue\n",
    "\n",
    "        # Tokenize for BLEU calculation (simple whitespace tokenizer)\n",
    "        ref_tokens = [reference.split()]  # List of references (only one here)\n",
    "        base_tokens = base_model.split()\n",
    "        gpt4o_tokens = gpt4o.split()\n",
    "        fine_tuned_tokens = fine_tuned.split()\n",
    "\n",
    "        # Calculate sentence-level BLEU scores (0-1 scale)\n",
    "        bleu_base = sentence_bleu(ref_tokens, base_tokens, smoothing_function=smooth)\n",
    "        bleu_gpt4o = sentence_bleu(ref_tokens, gpt4o_tokens, smoothing_function=smooth)\n",
    "        bleu_fine_tuned = sentence_bleu(ref_tokens, fine_tuned_tokens, smoothing_function=smooth)\n",
    "\n",
    "        # Save sentence-level scores for summary\n",
    "        base_model_scores.append(bleu_base)\n",
    "        gpt4o_scores.append(bleu_gpt4o)\n",
    "        fine_tuned_scores.append(bleu_fine_tuned)\n",
    "\n",
    "        # Count improvements\n",
    "        if bleu_fine_tuned > bleu_base:\n",
    "            fine_tuned_better_base += 1\n",
    "        if bleu_fine_tuned > bleu_gpt4o:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        # Append to corpus lists\n",
    "        corpus_references.append(ref_tokens)\n",
    "        corpus_base_model.append(base_tokens)\n",
    "        corpus_gpt4o.append(gpt4o_tokens)\n",
    "        corpus_fine_tuned.append(fine_tuned_tokens)\n",
    "\n",
    "        # Write per-sample results\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: BLEU = {bleu_fine_tuned:.4f}\\n\")\n",
    "        f.write(f\"base_model: BLEU = {bleu_base:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: BLEU = {bleu_gpt4o:.4f}\\n\\n\")\n",
    "\n",
    "    # Calculate corpus-level BLEU scores\n",
    "    corpus_bleu_base = corpus_bleu(corpus_references, corpus_base_model, smoothing_function=smooth)\n",
    "    corpus_bleu_gpt4o = corpus_bleu(corpus_references, corpus_gpt4o, smoothing_function=smooth)\n",
    "    corpus_bleu_fine_tuned = corpus_bleu(corpus_references, corpus_fine_tuned, smoothing_function=smooth)\n",
    "\n",
    "    # Write summary\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model (Mistral 7B): {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    f.write(\"Average Sentence-level BLEU Scores:\\n\")\n",
    "    f.write(f\"fine_tuned: {np.mean(fine_tuned_scores):.4f}\\n\")\n",
    "    f.write(f\"base_model: {np.mean(base_model_scores):.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {np.mean(gpt4o_scores):.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level BLEU Scores:\\n\")\n",
    "    f.write(f\"fine_tuned: {corpus_bleu_fine_tuned:.4f}\\n\")\n",
    "    f.write(f\"base_model: {corpus_bleu_base:.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {corpus_bleu_gpt4o:.4f}\\n\")\n",
    "\n",
    "print(\"BLEU scores saved to bleu_scores_nltk_model_1.txt\")\n"
   ],
   "id": "b869ffd15250d84e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores saved to bleu_scores_nltk_model_1.txt\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d6bf18a50535df94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BLUE score with nltk library",
   "id": "21eff9d1305bcc73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Model 1 BLUE score",
   "id": "71a2914f45302173"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:51:54.627978Z",
     "start_time": "2025-05-29T16:51:54.357919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load your data (replace with your actual file)\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 1 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation if needed\n",
    "    return text\n",
    "\n",
    "# Prepare smoothing function to avoid BLEU=0 for short sentences\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Lists for corpus BLEU calculation\n",
    "corpus_references = []\n",
    "corpus_base_model = []\n",
    "corpus_gpt4o = []\n",
    "corpus_fine_tuned = []\n",
    "\n",
    "# Open file for writing results\n",
    "with open(\"bleu_scores_nltk_model_1.txt\", \"w\") as f:\n",
    "\n",
    "    fine_tuned_better_base = 0\n",
    "    fine_tuned_better_gpt = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    fine_tuned_scores = []\n",
    "    base_model_scores = []\n",
    "    gpt4o_scores = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # Handle missing or non-string data and clean text\n",
    "        reference = clean_text(row['reference']) if pd.notna(row['reference']) else \"\"\n",
    "        base_model = clean_text(row['base_model']) if pd.notna(row['base_model']) else \"\"\n",
    "        gpt4o = clean_text(row['gpt4omini']) if pd.notna(row['gpt4omini']) else \"\"\n",
    "        fine_tuned = clean_text(row['fine_tuned']) if pd.notna(row['fine_tuned']) else \"\"\n",
    "\n",
    "        # Skip if reference or all models are empty\n",
    "        if reference == \"\" or (base_model == \"\" and gpt4o == \"\" and fine_tuned == \"\"):\n",
    "            continue\n",
    "\n",
    "        # Tokenize for BLEU calculation (simple whitespace tokenizer)\n",
    "        ref_tokens = [reference.split()]  # List of references (only one here)\n",
    "        base_tokens = base_model.split()\n",
    "        gpt4o_tokens = gpt4o.split()\n",
    "        fine_tuned_tokens = fine_tuned.split()\n",
    "\n",
    "        # Calculate sentence-level BLEU scores (0-1 scale)\n",
    "        bleu_base = sentence_bleu(ref_tokens, base_tokens, smoothing_function=smooth)\n",
    "        bleu_gpt4o = sentence_bleu(ref_tokens, gpt4o_tokens, smoothing_function=smooth)\n",
    "        bleu_fine_tuned = sentence_bleu(ref_tokens, fine_tuned_tokens, smoothing_function=smooth)\n",
    "\n",
    "        # Save sentence-level scores for summary\n",
    "        base_model_scores.append(bleu_base)\n",
    "        gpt4o_scores.append(bleu_gpt4o)\n",
    "        fine_tuned_scores.append(bleu_fine_tuned)\n",
    "\n",
    "        # Count improvements\n",
    "        if bleu_fine_tuned > bleu_base:\n",
    "            fine_tuned_better_base += 1\n",
    "        if bleu_fine_tuned > bleu_gpt4o:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        # Append to corpus lists\n",
    "        corpus_references.append(ref_tokens)\n",
    "        corpus_base_model.append(base_tokens)\n",
    "        corpus_gpt4o.append(gpt4o_tokens)\n",
    "        corpus_fine_tuned.append(fine_tuned_tokens)\n",
    "\n",
    "        # Write per-sample results\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: BLEU = {bleu_fine_tuned:.4f}\\n\")\n",
    "        f.write(f\"base_model: BLEU = {bleu_base:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: BLEU = {bleu_gpt4o:.4f}\\n\\n\")\n",
    "\n",
    "    # Calculate corpus-level BLEU scores\n",
    "    corpus_bleu_base = corpus_bleu(corpus_references, corpus_base_model, smoothing_function=smooth)\n",
    "    corpus_bleu_gpt4o = corpus_bleu(corpus_references, corpus_gpt4o, smoothing_function=smooth)\n",
    "    corpus_bleu_fine_tuned = corpus_bleu(corpus_references, corpus_fine_tuned, smoothing_function=smooth)\n",
    "\n",
    "    # Write summary\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model (Mistral 7B): {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    f.write(\"Average Sentence-level BLEU Scores:\\n\")\n",
    "    f.write(f\"fine_tuned: {np.mean(fine_tuned_scores):.4f}\\n\")\n",
    "    f.write(f\"base_model: {np.mean(base_model_scores):.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {np.mean(gpt4o_scores):.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level BLEU Scores:\\n\")\n",
    "    f.write(f\"fine_tuned: {corpus_bleu_fine_tuned:.4f}\\n\")\n",
    "    f.write(f\"base_model: {corpus_bleu_base:.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {corpus_bleu_gpt4o:.4f}\\n\")\n",
    "\n",
    "print(\"BLEU scores saved to bleu_scores_nltk_model_1.txt\")\n"
   ],
   "id": "e700dea906f68ee9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores saved to bleu_scores_nltk_model_1.txt\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2 BLUE score",
   "id": "1aa6c85b1b601da9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:52:12.443632Z",
     "start_time": "2025-05-29T16:52:12.178763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load your data (replace with your actual file)\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 2 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation if needed\n",
    "    return text\n",
    "\n",
    "# Prepare smoothing function to avoid BLEU=0 for short sentences\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Lists for corpus BLEU calculation\n",
    "corpus_references = []\n",
    "corpus_base_model = []\n",
    "corpus_gpt4o = []\n",
    "corpus_fine_tuned = []\n",
    "\n",
    "# Open file for writing results\n",
    "with open(\"bleu_scores_nltk_model_2.txt\", \"w\") as f:\n",
    "\n",
    "    fine_tuned_better_base = 0\n",
    "    fine_tuned_better_gpt = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    fine_tuned_scores = []\n",
    "    base_model_scores = []\n",
    "    gpt4o_scores = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # Handle missing or non-string data and clean text\n",
    "        reference = clean_text(row['reference']) if pd.notna(row['reference']) else \"\"\n",
    "        base_model = clean_text(row['base_model']) if pd.notna(row['base_model']) else \"\"\n",
    "        gpt4o = clean_text(row['gpt4omini']) if pd.notna(row['gpt4omini']) else \"\"\n",
    "        fine_tuned = clean_text(row['fine_tuned']) if pd.notna(row['fine_tuned']) else \"\"\n",
    "\n",
    "        # Skip if reference or all models are empty\n",
    "        if reference == \"\" or (base_model == \"\" and gpt4o == \"\" and fine_tuned == \"\"):\n",
    "            continue\n",
    "\n",
    "        # Tokenize for BLEU calculation (simple whitespace tokenizer)\n",
    "        ref_tokens = [reference.split()]  # List of references (only one here)\n",
    "        base_tokens = base_model.split()\n",
    "        gpt4o_tokens = gpt4o.split()\n",
    "        fine_tuned_tokens = fine_tuned.split()\n",
    "\n",
    "        # Calculate sentence-level BLEU scores (0-1 scale)\n",
    "        bleu_base = sentence_bleu(ref_tokens, base_tokens, smoothing_function=smooth)\n",
    "        bleu_gpt4o = sentence_bleu(ref_tokens, gpt4o_tokens, smoothing_function=smooth)\n",
    "        bleu_fine_tuned = sentence_bleu(ref_tokens, fine_tuned_tokens, smoothing_function=smooth)\n",
    "\n",
    "        # Save sentence-level scores for summary\n",
    "        base_model_scores.append(bleu_base)\n",
    "        gpt4o_scores.append(bleu_gpt4o)\n",
    "        fine_tuned_scores.append(bleu_fine_tuned)\n",
    "\n",
    "        # Count improvements\n",
    "        if bleu_fine_tuned > bleu_base:\n",
    "            fine_tuned_better_base += 1\n",
    "        if bleu_fine_tuned > bleu_gpt4o:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        # Append to corpus lists\n",
    "        corpus_references.append(ref_tokens)\n",
    "        corpus_base_model.append(base_tokens)\n",
    "        corpus_gpt4o.append(gpt4o_tokens)\n",
    "        corpus_fine_tuned.append(fine_tuned_tokens)\n",
    "\n",
    "        # Write per-sample results\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: BLEU = {bleu_fine_tuned:.4f}\\n\")\n",
    "        f.write(f\"base_model: BLEU = {bleu_base:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: BLEU = {bleu_gpt4o:.4f}\\n\\n\")\n",
    "\n",
    "    # Calculate corpus-level BLEU scores\n",
    "    corpus_bleu_base = corpus_bleu(corpus_references, corpus_base_model, smoothing_function=smooth)\n",
    "    corpus_bleu_gpt4o = corpus_bleu(corpus_references, corpus_gpt4o, smoothing_function=smooth)\n",
    "    corpus_bleu_fine_tuned = corpus_bleu(corpus_references, corpus_fine_tuned, smoothing_function=smooth)\n",
    "\n",
    "    # Write summary\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model (Mistral 7B): {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    f.write(\"Average Sentence-level BLEU Scores:\\n\")\n",
    "    f.write(f\"fine_tuned: {np.mean(fine_tuned_scores):.4f}\\n\")\n",
    "    f.write(f\"base_model: {np.mean(base_model_scores):.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {np.mean(gpt4o_scores):.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level BLEU Scores:\\n\")\n",
    "    f.write(f\"fine_tuned: {corpus_bleu_fine_tuned:.4f}\\n\")\n",
    "    f.write(f\"base_model: {corpus_bleu_base:.4f}\\n\")\n",
    "    f.write(f\"gpt4omini: {corpus_bleu_gpt4o:.4f}\\n\")\n",
    "\n",
    "print(\"BLEU scores saved to bleu_scores_nltk_model_2.txt\")\n"
   ],
   "id": "dff4479c95ca18a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores saved to bleu_scores_nltk_model_2.txt\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T16:44:02.849240Z",
     "start_time": "2025-05-29T16:44:02.846543Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "70285d3b1cd73450",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec97310c833c87a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
