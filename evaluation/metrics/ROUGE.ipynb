{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ROUGE score with rouge_score library",
   "id": "b8c4c65446e8d1d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1 ROUGE score",
   "id": "ded49570ad4b775b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T19:46:13.691415Z",
     "start_time": "2025-05-29T19:46:09.047881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation if needed\n",
    "    return text\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "\n",
    "# Clean and rename columns\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 1 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Bootstrap aggregators\n",
    "aggregator_fine = scoring.BootstrapAggregator()\n",
    "aggregator_base = scoring.BootstrapAggregator()\n",
    "aggregator_gpt = scoring.BootstrapAggregator()\n",
    "\n",
    "fine_tuned_better_base = 0\n",
    "fine_tuned_better_gpt = 0\n",
    "total_samples = 0\n",
    "\n",
    "with open(\"rouge_scores_model_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        # Clean the text fields\n",
    "        reference = clean_text(row['reference']) if pd.notna(row['reference']) else \"\"\n",
    "        base_model = clean_text(row['base_model']) if pd.notna(row['base_model']) else \"\"\n",
    "        gpt4o = clean_text(row['gpt4omini']) if pd.notna(row['gpt4omini']) else \"\"\n",
    "        fine_tuned = clean_text(row['fine_tuned']) if pd.notna(row['fine_tuned']) else \"\"\n",
    "\n",
    "        if reference == \"\" or (base_model == \"\" and gpt4o == \"\" and fine_tuned == \"\"):\n",
    "            continue\n",
    "\n",
    "        # Score each model\n",
    "        score_base = scorer.score(reference, base_model)\n",
    "        score_gpt = scorer.score(reference, gpt4o)\n",
    "        score_fine = scorer.score(reference, fine_tuned)\n",
    "\n",
    "        aggregator_base.add_scores(score_base)\n",
    "        aggregator_gpt.add_scores(score_gpt)\n",
    "        aggregator_fine.add_scores(score_fine)\n",
    "\n",
    "        # Compare ROUGE-L F1\n",
    "        if score_fine['rougeL'].fmeasure > score_base['rougeL'].fmeasure:\n",
    "            fine_tuned_better_base += 1\n",
    "        if score_fine['rougeL'].fmeasure > score_gpt['rougeL'].fmeasure:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        # Write sample results\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: ROUGE-L = {score_fine['rougeL'].fmeasure:.4f}\\n\")\n",
    "        f.write(f\"base_model: ROUGE-L = {score_base['rougeL'].fmeasure:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: ROUGE-L = {score_gpt['rougeL'].fmeasure:.4f}\\n\\n\")\n",
    "\n",
    "    # Aggregate results\n",
    "    results_fine = aggregator_fine.aggregate()\n",
    "    results_base = aggregator_base.aggregate()\n",
    "    results_gpt = aggregator_gpt.aggregate()\n",
    "\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model: {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    def write_agg_scores(name, scores):\n",
    "        f.write(f\"{name}:\\n\")\n",
    "        f.write(f\"  ROUGE-1: {scores['rouge1'].mid.fmeasure:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-2: {scores['rouge2'].mid.fmeasure:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-L: {scores['rougeL'].mid.fmeasure:.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level ROUGE Scores:\\n\")\n",
    "    write_agg_scores(\"fine_tuned\", results_fine)\n",
    "    write_agg_scores(\"base_model\", results_base)\n",
    "    write_agg_scores(\"gpt4omini\", results_gpt)\n",
    "\n",
    "print(\"ROUGE scores saved to rouge_scores_model_1.txt\")\n"
   ],
   "id": "eb757c47c2aec815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores saved to rouge_scores_model_1.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2 ROUGE score",
   "id": "74d3f3d7bb9b8a3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T19:46:15.176702Z",
     "start_time": "2025-05-29T19:46:13.691415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation if needed\n",
    "    return text\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "\n",
    "# Clean and rename columns\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 2 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Bootstrap aggregators\n",
    "aggregator_fine = scoring.BootstrapAggregator()\n",
    "aggregator_base = scoring.BootstrapAggregator()\n",
    "aggregator_gpt = scoring.BootstrapAggregator()\n",
    "\n",
    "fine_tuned_better_base = 0\n",
    "fine_tuned_better_gpt = 0\n",
    "total_samples = 0\n",
    "\n",
    "with open(\"rouge_scores_model_2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        # Clean the text fields\n",
    "        reference = clean_text(row['reference']) if pd.notna(row['reference']) else \"\"\n",
    "        base_model = clean_text(row['base_model']) if pd.notna(row['base_model']) else \"\"\n",
    "        gpt4o = clean_text(row['gpt4omini']) if pd.notna(row['gpt4omini']) else \"\"\n",
    "        fine_tuned = clean_text(row['fine_tuned']) if pd.notna(row['fine_tuned']) else \"\"\n",
    "\n",
    "        if reference == \"\" or (base_model == \"\" and gpt4o == \"\" and fine_tuned == \"\"):\n",
    "            continue\n",
    "\n",
    "        # Score each model\n",
    "        score_base = scorer.score(reference, base_model)\n",
    "        score_gpt = scorer.score(reference, gpt4o)\n",
    "        score_fine = scorer.score(reference, fine_tuned)\n",
    "\n",
    "        aggregator_base.add_scores(score_base)\n",
    "        aggregator_gpt.add_scores(score_gpt)\n",
    "        aggregator_fine.add_scores(score_fine)\n",
    "\n",
    "        # Compare ROUGE-L F1\n",
    "        if score_fine['rougeL'].fmeasure > score_base['rougeL'].fmeasure:\n",
    "            fine_tuned_better_base += 1\n",
    "        if score_fine['rougeL'].fmeasure > score_gpt['rougeL'].fmeasure:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        # Write sample results\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: ROUGE-L = {score_fine['rougeL'].fmeasure:.4f}\\n\")\n",
    "        f.write(f\"base_model: ROUGE-L = {score_base['rougeL'].fmeasure:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: ROUGE-L = {score_gpt['rougeL'].fmeasure:.4f}\\n\\n\")\n",
    "\n",
    "    # Aggregate results\n",
    "    results_fine = aggregator_fine.aggregate()\n",
    "    results_base = aggregator_base.aggregate()\n",
    "    results_gpt = aggregator_gpt.aggregate()\n",
    "\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model: {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    def write_agg_scores(name, scores):\n",
    "        f.write(f\"{name}:\\n\")\n",
    "        f.write(f\"  ROUGE-1: {scores['rouge1'].mid.fmeasure:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-2: {scores['rouge2'].mid.fmeasure:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-L: {scores['rougeL'].mid.fmeasure:.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level ROUGE Scores:\\n\")\n",
    "    write_agg_scores(\"fine_tuned\", results_fine)\n",
    "    write_agg_scores(\"base_model\", results_base)\n",
    "    write_agg_scores(\"gpt4omini\", results_gpt)\n",
    "\n",
    "print(\"ROUGE scores saved to rouge_scores_model_2.txt\")\n"
   ],
   "id": "120d49bccf1c6851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores saved to rouge_scores_model_2.txt\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ROUGE score with Hugging Face datasets library",
   "id": "2c9b4527a2ed16b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1 ROUGE score",
   "id": "5d66620480c17a02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T17:34:27.448982Z",
     "start_time": "2025-05-29T17:33:34.845351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import evaluate\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Load ROUGE once outside the loop\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 1 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Limit to first 60 valid rows\n",
    "df = df.dropna(subset=['reference', 'base_model', 'gpt4omini', 'fine_tuned'])\n",
    "df = df.head(60)\n",
    "\n",
    "# Initialize counters and lists\n",
    "fine_tuned_better_base = 0\n",
    "fine_tuned_better_gpt = 0\n",
    "total_samples = 0\n",
    "references, base_preds, gpt_preds, fine_preds = [], [], [], []\n",
    "\n",
    "with open(\"rouge_with_huggingface_model_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        reference = clean_text(row['reference'])\n",
    "        base_model = clean_text(row['base_model'])\n",
    "        gpt4o = clean_text(row['gpt4omini'])\n",
    "        fine_tuned = clean_text(row['fine_tuned'])\n",
    "\n",
    "        references.append(reference)\n",
    "        base_preds.append(base_model)\n",
    "        gpt_preds.append(gpt4o)\n",
    "        fine_preds.append(fine_tuned)\n",
    "\n",
    "        # Compute individual ROUGE-L scores\n",
    "        fine_score = rouge.compute(predictions=[fine_tuned], references=[reference])\n",
    "        base_score = rouge.compute(predictions=[base_model], references=[reference])\n",
    "        gpt_score = rouge.compute(predictions=[gpt4o], references=[reference])\n",
    "\n",
    "        if fine_score['rougeL'] > base_score['rougeL']:\n",
    "            fine_tuned_better_base += 1\n",
    "        if fine_score['rougeL'] > gpt_score['rougeL']:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: ROUGE-L = {fine_score['rougeL']:.4f}\\n\")\n",
    "        f.write(f\"base_model: ROUGE-L = {base_score['rougeL']:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: ROUGE-L = {gpt_score['rougeL']:.4f}\\n\\n\")\n",
    "\n",
    "    # Corpus-level scores\n",
    "    score_fine = rouge.compute(predictions=fine_preds, references=references)\n",
    "    score_base = rouge.compute(predictions=base_preds, references=references)\n",
    "    score_gpt = rouge.compute(predictions=gpt_preds, references=references)\n",
    "\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model: {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    def write_agg_scores(name, scores):\n",
    "        f.write(f\"{name}:\\n\")\n",
    "        f.write(f\"  ROUGE-1: {scores['rouge1']:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-2: {scores['rouge2']:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-L: {scores['rougeL']:.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level ROUGE Scores:\\n\")\n",
    "    write_agg_scores(\"fine_tuned\", score_fine)\n",
    "    write_agg_scores(\"base_model\", score_base)\n",
    "    write_agg_scores(\"gpt4omini\", score_gpt)\n",
    "\n",
    "print(\"ROUGE scores saved to rouge_with_huggingface_model_1.txt\")\n"
   ],
   "id": "29e27bcb09c623d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores saved to rouge_with_huggingface_model_1.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2 ROUGE score",
   "id": "8a107db9e18c2c80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T17:35:27.543407Z",
     "start_time": "2025-05-29T17:34:40.944535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import evaluate\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Load ROUGE once outside the loop\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(r\"C:\\Users\\LENOVO\\Desktop\\Sample.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={\n",
    "    'Expected Answer': 'reference',\n",
    "    'Initial Answer from Base Model (Mistral 7B)': 'base_model',\n",
    "    'Initial Answer from LLM (gpt-4o-mini)': 'gpt4omini',\n",
    "    'Finetuned Mistral Model 2 (100 Tokens)': 'fine_tuned'\n",
    "})\n",
    "\n",
    "# Limit to first 60 valid rows\n",
    "df = df.dropna(subset=['reference', 'base_model', 'gpt4omini', 'fine_tuned'])\n",
    "df = df.head(60)\n",
    "\n",
    "# Initialize counters and lists\n",
    "fine_tuned_better_base = 0\n",
    "fine_tuned_better_gpt = 0\n",
    "total_samples = 0\n",
    "references, base_preds, gpt_preds, fine_preds = [], [], [], []\n",
    "\n",
    "with open(\"rouge_with_huggingface_model_2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        reference = clean_text(row['reference'])\n",
    "        base_model = clean_text(row['base_model'])\n",
    "        gpt4o = clean_text(row['gpt4omini'])\n",
    "        fine_tuned = clean_text(row['fine_tuned'])\n",
    "\n",
    "        references.append(reference)\n",
    "        base_preds.append(base_model)\n",
    "        gpt_preds.append(gpt4o)\n",
    "        fine_preds.append(fine_tuned)\n",
    "\n",
    "        # Compute individual ROUGE-L scores\n",
    "        fine_score = rouge.compute(predictions=[fine_tuned], references=[reference])\n",
    "        base_score = rouge.compute(predictions=[base_model], references=[reference])\n",
    "        gpt_score = rouge.compute(predictions=[gpt4o], references=[reference])\n",
    "\n",
    "        if fine_score['rougeL'] > base_score['rougeL']:\n",
    "            fine_tuned_better_base += 1\n",
    "        if fine_score['rougeL'] > gpt_score['rougeL']:\n",
    "            fine_tuned_better_gpt += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "        f.write(f\"Sample {i+1}:\\n\")\n",
    "        f.write(f\"fine_tuned: ROUGE-L = {fine_score['rougeL']:.4f}\\n\")\n",
    "        f.write(f\"base_model: ROUGE-L = {base_score['rougeL']:.4f}\\n\")\n",
    "        f.write(f\"gpt4omini: ROUGE-L = {gpt_score['rougeL']:.4f}\\n\\n\")\n",
    "\n",
    "    # Corpus-level scores\n",
    "    score_fine = rouge.compute(predictions=fine_preds, references=references)\n",
    "    score_base = rouge.compute(predictions=base_preds, references=references)\n",
    "    score_gpt = rouge.compute(predictions=gpt_preds, references=references)\n",
    "\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Fine-tuned > Base Model: {fine_tuned_better_base}/{total_samples} ({fine_tuned_better_base/total_samples*100:.2f}%)\\n\")\n",
    "    f.write(f\"Fine-tuned > GPT-4o-mini: {fine_tuned_better_gpt}/{total_samples} ({fine_tuned_better_gpt/total_samples*100:.2f}%)\\n\\n\")\n",
    "\n",
    "    def write_agg_scores(name, scores):\n",
    "        f.write(f\"{name}:\\n\")\n",
    "        f.write(f\"  ROUGE-1: {scores['rouge1']:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-2: {scores['rouge2']:.4f}\\n\")\n",
    "        f.write(f\"  ROUGE-L: {scores['rougeL']:.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Corpus-level ROUGE Scores:\\n\")\n",
    "    write_agg_scores(\"fine_tuned\", score_fine)\n",
    "    write_agg_scores(\"base_model\", score_base)\n",
    "    write_agg_scores(\"gpt4omini\", score_gpt)\n",
    "\n",
    "print(\"ROUGE scores saved to rouge_with_huggingface_model_2.txt\")\n"
   ],
   "id": "ba28f4d99cb35527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores saved to rouge_with_huggingface_model_2.txt\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44276869edbb8e89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
